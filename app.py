import gradio as gr
import json
import os
import random
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple
import subprocess
import sys
import tempfile
import threading
import time
from PIL import Image

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
try:
    from src.llm_client import LLMClient
    from src.image_generate_class import ImageGenerator
    from src.vlm_validator import VLMValidator
except ImportError as e:
    print(f"âš ï¸ å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    print("è¯·ç¡®ä¿srcç›®å½•ä¸‹æœ‰ç›¸åº”çš„æ¨¡å—æ–‡ä»¶")

class VideoGenerationPipeline:
    def __init__(self):
        self.current_project_dir = None
        self.llm_client = None
        self.image_generator = None
        self.vlm_validator = None
        self.load_prompts_config()
        
        # è®¾ç½®æœ¬åœ°ä¸´æ—¶ç›®å½•
        self.temp_dir = Path("./tmp")
        self.temp_dir.mkdir(exist_ok=True)
        
        # å›¾åƒç”ŸæˆçŠ¶æ€ç®¡ç†
        self.image_generation_status = {}
        self.current_prompts = []
        self.current_images = []
        
    def load_prompts_config(self):
        """åŠ è½½prompté…ç½®"""
        config_path = Path("config/prompts.json")
        config_path.parent.mkdir(exist_ok=True)
        
        default_config = {
            "system_prompts": {
                "content_generation": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çŸ­è§†é¢‘å‰§æœ¬å®¶å’ŒAIç»˜ç”»/è§†é¢‘æç¤ºè¯å·¥ç¨‹å¸ˆã€‚",
                "image_validation": "ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å›¾åƒåˆ†æå¸ˆï¼Œè¯·ä»”ç»†è§‚å¯Ÿå›¾åƒå†…å®¹ã€‚"
            },
            "validation_prompt": "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ï¼Œæè¿°å›¾ç‰‡ä¸­çš„ä¸»è¦å†…å®¹ã€åœºæ™¯ã€äººç‰©åŠ¨ä½œå’Œæƒ…æ„Ÿè¡¨è¾¾ã€‚åˆ¤æ–­æ˜¯å¦ç¬¦åˆé¢„æœŸçš„æç¤ºè¯è¦æ±‚ã€‚"
        }
        
        if not config_path.exists():
            with open(config_path, 'w', encoding='utf-8') as f:
                json.dump(default_config, f, ensure_ascii=False, indent=2)
        
        with open(config_path, 'r', encoding='utf-8') as f:
            self.prompts_config = json.load(f)

    def generate_content(self, search_query: str, api_key: str, base_url: str, 
                        model_name: str, scene_count: int) -> Tuple[str, str, str, str, str, str]:
        """ç”Ÿæˆå†…å®¹çš„äº”ç§ç»“æœ"""
        try:
            if not all([search_query, api_key, model_name]):
                return "âŒ è¯·å¡«å†™å®Œæ•´çš„æœç´¢è¯ã€API Keyå’Œæ¨¡å‹åç§°", "", "", "", "", ""
            
            # åˆå§‹åŒ–LLMå®¢æˆ·ç«¯
            self.llm_client = LLMClient(api_key=api_key, base_url=base_url, model=model_name)
            
            # åˆ›å»ºé¡¹ç›®ç›®å½•
            safe_search_query = "".join(c for c in search_query if c.isalnum() or c in (' ', '_', '-')).strip()
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            project_name = f"{safe_search_query}_{timestamp}"
            self.current_project_dir = Path("generated_video_content") / project_name
            self.current_project_dir.mkdir(parents=True, exist_ok=True)
            
            # ä¿®æ”¹LLMå®¢æˆ·ç«¯ä»¥æ”¯æŒè‡ªå®šä¹‰åœºæ™¯æ•°é‡
            result = self._generate_custom_content(search_query, scene_count)
            
            if result["success"]:
                data = result["data"]
                self.current_project_dir = Path(data["output_folder"])
                
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºæ¢è¡Œåˆ†éš”çš„å­—ç¬¦ä¸²
                img_prompts_text = "\n".join(data["img2img_prompts"])
                vid_prompts_text = "\n".join(data["img2vid_prompts"])
                narrations_text = "\n".join(data["narrations"])
                
                # æ›´æ–°å½“å‰æç¤ºè¯åˆ—è¡¨
                self.current_prompts = data["img2img_prompts"]
                self.current_images = [None] * len(self.current_prompts)
                
                print(f"âœ… å†…å®¹ç”ŸæˆæˆåŠŸï¼")
                print(f"é¡¹ç›®ç›®å½•ï¼š{self.current_project_dir}")
                print(f"ç”Ÿæˆäº† {len(self.current_prompts)} ä¸ªåœºæ™¯")
                
                return (
                    f"âœ… å†…å®¹ç”ŸæˆæˆåŠŸï¼\né¡¹ç›®ç›®å½•ï¼š{self.current_project_dir}",
                    img_prompts_text,
                    vid_prompts_text,
                    narrations_text,
                    data["business_points"],
                    data["service_overview"]
                )
            else:
                return f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{result['error']}", "", "", "", "", ""
                
        except Exception as e:
            return f"âŒ é”™è¯¯ï¼š{str(e)}", "", "", "", "", ""

    def _generate_custom_content(self, search_query: str, scene_count: int) -> Dict[str, Any]:
        """ç”Ÿæˆè‡ªå®šä¹‰åœºæ™¯æ•°é‡çš„å†…å®¹"""
        try:
            prompt = self._build_custom_prompt(search_query, scene_count)
            
            response = self.llm_client.client.chat.completions.create(
                model=self.llm_client.model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=4196,
                temperature=0.7
            )
            
            content = response.choices[0].message.content
            parsed_content = self._parse_generated_content(content, scene_count)
            
            # ä¿å­˜åˆ°é¡¹ç›®ç›®å½•
            self._save_content_to_project(parsed_content)
            parsed_content["output_folder"] = str(self.current_project_dir)
            
            return {"success": True, "data": parsed_content}
            
        except Exception as e:
            return {"success": False, "error": str(e)}

    def _build_custom_prompt(self, search_query: str, scene_count: int) -> str:
        """æ„å»ºè‡ªå®šä¹‰åœºæ™¯æ•°é‡çš„prompt"""
        style_prefix = "Anime style, high quality, consistent character design, "
        
        return f"""
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„çŸ­è§†é¢‘å‰§æœ¬å®¶å’ŒAIç»˜ç”»/è§†é¢‘æç¤ºè¯å·¥ç¨‹å¸ˆã€‚
è¯·æ ¹æ®æœç´¢è¯"{search_query}"ï¼Œä¸ºæˆ‘ç”Ÿæˆä¸€ä¸ªå¼•äººå…¥èƒœçš„çŸ­è§†é¢‘çš„å®Œæ•´å†…å®¹æ–¹æ¡ˆï¼Œå…±{scene_count}ä¸ªåœºæ™¯ã€‚

## æ ¸å¿ƒå™äº‹åŸåˆ™ï¼š
1. å™äº‹è¿è´¯æ€§ï¼šæ‰€æœ‰{scene_count}ä¸ªåœºæ™¯å¿…é¡»æ„æˆä¸€ä¸ªå®Œæ•´ã€è¿è´¯çš„æ•…äº‹
2. æ—¶åºä¸€è‡´æ€§ï¼šåœºæ™¯å¿…é¡»éµå¾ªä¸¥æ ¼çš„æ—¶é—´é¡ºåº

## ç”Ÿæˆå†…å®¹è¦æ±‚ï¼š

### 1. å›¾ç‰‡ç”Ÿæˆæç¤ºè¯ï¼ˆ{scene_count}ä¸ªåœºæ™¯ï¼‰
- æ¯ä¸ªæç¤ºè¯ç”¨è‹±æ–‡ç¼–å†™ï¼Œé•¿åº¦åœ¨40-60ä¸ªå•è¯ä¹‹é—´
- å¿…é¡»ä»¥"{style_prefix}"å¼€å¤´
- åŒ…å«è¯¦ç»†çš„ä¸»ä½“ã€åœºæ™¯ã€åŠ¨ä½œã€é•œå¤´è¯­è¨€å’Œæ°›å›´æè¿°

### 2. è§†é¢‘ç”Ÿæˆæç¤ºè¯ï¼ˆ{scene_count}ä¸ªåœºæ™¯ï¼‰
- æ¯ä¸ªæç¤ºè¯ç”¨è‹±æ–‡ç¼–å†™ï¼Œé•¿åº¦åœ¨50-80ä¸ªå•è¯ä¹‹é—´
- å¿…é¡»ä»¥"{style_prefix}"å¼€å¤´
- å¿…é¡»åŒ…å«å…·ä½“çš„é•œå¤´è¿åŠ¨æè¿°

### 3. æ—ç™½æ–‡æœ¬ï¼ˆ{scene_count}æ®µï¼‰
- æ¯æ®µä¸è¶…è¿‡15ä¸ªä¸­æ–‡å­—
- è¯­è¨€ç®€æ´æœ‰åŠ›ï¼Œå¯Œæœ‰æ„ŸæŸ“åŠ›

### 4. è§†é¢‘ä¸šåŠ¡ç‚¹
- 3-5ä¸ªæ ¸å¿ƒä»·å€¼ç‚¹æˆ–æ•…äº‹ä¸»æ—¨
- ä¸­æ–‡æè¿°

### 5. æœåŠ¡æ¦‚è¿°
- 100å­—ä»¥å†…çš„ä¸­æ–‡æè¿°
- è¯´æ˜è§†é¢‘çš„æ•´ä½“æ•…äº‹ã€ä»·å€¼å’Œç›®æ ‡å—ä¼—

è¯·ä¸¥æ ¼æŒ‰ç…§ä»¥ä¸‹JSONæ ¼å¼è¿”å›ï¼š
```json
{{
    "img2img_prompts": [
        "{style_prefix}åœºæ™¯1...",
        "{style_prefix}åœºæ™¯2...",
        ...
    ],
    "img2vid_prompts": [
        "{style_prefix}é•œå¤´è¿åŠ¨ + åœºæ™¯1...",
        "{style_prefix}é•œå¤´è¿åŠ¨ + åœºæ™¯2...",
        ...
    ],
    "narrations": [
        "æ—ç™½1",
        "æ—ç™½2",
        ...
    ],
    "business_points": "ä¸šåŠ¡ç‚¹æè¿°...",
    "service_overview": "æœåŠ¡æ¦‚è¿°..."
}}```
"""

    def _parse_generated_content(self, content: str, scene_count: int) -> Dict[str, Any]:
        """è§£æç”Ÿæˆçš„å†…å®¹"""
        try:
            import re
            json_match = re.search(r'```json\s*([\s\S]*?)\s*```', content)
            if json_match:
                json_str = json_match.group(1)
            else:
                json_str = content
                
            data = json.loads(json_str)
            
            # ç¡®ä¿åˆ—è¡¨é•¿åº¦ä¸ºæŒ‡å®šåœºæ™¯æ•°
            for field in ["img2img_prompts", "img2vid_prompts", "narrations"]:
                if field in data:
                    while len(data[field]) < scene_count:
                        data[field].append(f"Default {field} {len(data[field])+1}")
                    data[field] = data[field][:scene_count]
                else:
                    data[field] = [f"Default {field} {i+1}" for i in range(scene_count)]
            
            return data
            
        except Exception as e:
            # è¿”å›é»˜è®¤æ•°æ®
            return {
                "img2img_prompts": [f"Anime style scene {i+1}" for i in range(scene_count)],
                "img2vid_prompts": [f"Anime style video scene {i+1}" for i in range(scene_count)],
                "narrations": [f"æ—ç™½{i+1}" for i in range(scene_count)],
                "business_points": "é»˜è®¤ä¸šåŠ¡ç‚¹",
                "service_overview": "é»˜è®¤æœåŠ¡æ¦‚è¿°"
            }

    def _save_content_to_project(self, content: Dict[str, Any]):
        """ä¿å­˜å†…å®¹åˆ°é¡¹ç›®ç›®å½•"""
        # ä¿å­˜å„ç§æç¤ºè¯æ–‡ä»¶
        with open(self.current_project_dir / "img2img_prompts.txt", "w", encoding="utf-8") as f:
            for prompt in content["img2img_prompts"]:
                f.write(f"{prompt}\n")
                
        with open(self.current_project_dir / "img2vid_prompts.txt", "w", encoding="utf-8") as f:
            for prompt in content["img2vid_prompts"]:
                f.write(f"{prompt}\n")
                
        with open(self.current_project_dir / "narrations.txt", "w", encoding="utf-8") as f:
            for narration in content["narrations"]:
                f.write(f"{narration}\n")
                
        with open(self.current_project_dir / "business_points.txt", "w", encoding="utf-8") as f:
            f.write(content["business_points"])
            
        with open(self.current_project_dir / "service_overview.txt", "w", encoding="utf-8") as f:
            f.write(content["service_overview"])
            
        with open(self.current_project_dir / "full_output.json", "w", encoding="utf-8") as f:
            json.dump(content, f, ensure_ascii=False, indent=2)

    def save_edited_content(self, img_prompts: str, vid_prompts: str, 
                           narrations: str, business_points: str, service_overview: str) -> str:
        """ä¿å­˜ç¼–è¾‘åçš„å†…å®¹"""
        try:
            if not self.current_project_dir:
                return "âŒ æ²¡æœ‰å½“å‰é¡¹ç›®ï¼Œè¯·å…ˆç”Ÿæˆå†…å®¹"
            
            # å°†å­—ç¬¦ä¸²è½¬æ¢ä¸ºåˆ—è¡¨
            img_list = [line.strip() for line in img_prompts.split('\n') if line.strip()]
            vid_list = [line.strip() for line in vid_prompts.split('\n') if line.strip()]
            nar_list = [line.strip() for line in narrations.split('\n') if line.strip()]
            
            content = {
                "img2img_prompts": img_list,
                "img2vid_prompts": vid_list,
                "narrations": nar_list,
                "business_points": business_points,
                "service_overview": service_overview
            }
            
            # æ›´æ–°å½“å‰æç¤ºè¯
            self.current_prompts = img_list
            self.current_images = [None] * len(self.current_prompts)
            
            self._save_content_to_project(content)
            return f"âœ… å†…å®¹å·²ä¿å­˜åˆ°ï¼š{self.current_project_dir}"
            
        except Exception as e:
            return f"âŒ ä¿å­˜å¤±è´¥ï¼š{str(e)}"

    def regenerate_content(self, search_query: str, api_key: str, base_url: str, 
                          model_name: str, scene_count: int, dissatisfaction: str) -> Tuple[str, str, str, str, str, str]:
        """é‡æ–°ç”Ÿæˆå†…å®¹"""
        try:
            if not dissatisfaction.strip():
                return self.generate_content(search_query, api_key, base_url, model_name, scene_count)
            
            # åœ¨åŸæœ‰promptåŸºç¡€ä¸ŠåŠ å…¥ä¸æ»¡æ„çš„ç‚¹
            modified_prompt = self._build_custom_prompt(search_query, scene_count) + f"\n\nè¯·ç‰¹åˆ«æ³¨æ„é¿å…ä»¥ä¸‹é—®é¢˜ï¼š{dissatisfaction}"
            
            self.llm_client = LLMClient(api_key=api_key, base_url=base_url, model=model_name)
            
            response = self.llm_client.client.chat.completions.create(
                model=self.llm_client.model,
                messages=[{"role": "user", "content": modified_prompt}],
                max_tokens=4196,
                temperature=0.8  # ç¨å¾®å¢åŠ éšæœºæ€§
            )
            
            content = response.choices[0].message.content
            parsed_content = self._parse_generated_content(content, scene_count)
            
            # è¦†ç›–ä¿å­˜åˆ°åŒä¸€ä¸ªé¡¹ç›®ç›®å½•
            if self.current_project_dir:
                self._save_content_to_project(parsed_content)
                
                # æ›´æ–°å½“å‰æç¤ºè¯
                self.current_prompts = parsed_content["img2img_prompts"]
                self.current_images = [None] * len(self.current_prompts)
                
                # å°†åˆ—è¡¨è½¬æ¢ä¸ºæ¢è¡Œåˆ†éš”çš„å­—ç¬¦ä¸²
                img_prompts_text = "\n".join(parsed_content["img2img_prompts"])
                vid_prompts_text = "\n".join(parsed_content["img2vid_prompts"])
                narrations_text = "\n".join(parsed_content["narrations"])
                
                return (
                    f"âœ… å†…å®¹é‡æ–°ç”ŸæˆæˆåŠŸï¼\nå·²è¦†ç›–ä¿å­˜åˆ°ï¼š{self.current_project_dir}",
                    img_prompts_text,
                    vid_prompts_text,
                    narrations_text,
                    parsed_content["business_points"],
                    parsed_content["service_overview"]
                )
            else:
                return "âŒ æ²¡æœ‰å½“å‰é¡¹ç›®ç›®å½•", "", "", "", "", ""
                
        except Exception as e:
            return f"âŒ é‡æ–°ç”Ÿæˆå¤±è´¥ï¼š{str(e)}", "", "", "", "", ""

    def initialize_image_generator(self, ip_adapter_path: str, base_model: str, 
                                  image_encoder_path: str, image_encoder_2_path: str, use_offload: bool):
        """åˆå§‹åŒ–å›¾ç‰‡ç”Ÿæˆå™¨"""
        try:
            # å°è¯•åˆ›å»ºImageGeneratorå®ä¾‹
            try:
                self.image_generator = ImageGenerator(model_type="flux", use_offload=use_offload)
            except TypeError:
                try:
                    self.image_generator = ImageGenerator(model_type="flux")
                except:
                    self.image_generator = ImageGenerator()
            
            # æ›´æ–°é…ç½®
            if hasattr(self.image_generator, 'update_config'):
                self.image_generator.update_config(
                    ip_adapter_path=ip_adapter_path,
                    base_model=base_model,
                    image_encoder_path=image_encoder_path,
                    image_encoder_2_path=image_encoder_2_path,
                    use_offload=use_offload
                )
            else:
                # ç›´æ¥è®¾ç½®å±æ€§
                self.image_generator.ip_adapter_path = ip_adapter_path
                self.image_generator.base_model = base_model
                self.image_generator.image_encoder_path = image_encoder_path
                self.image_generator.image_encoder_2_path = image_encoder_2_path
                if hasattr(self.image_generator, 'use_offload'):
                    self.image_generator.use_offload = use_offload
            
            return True, "å›¾ç‰‡ç”Ÿæˆå™¨åˆå§‹åŒ–æˆåŠŸ"
            
        except Exception as e:
            return False, f"å›¾ç‰‡ç”Ÿæˆå™¨åˆå§‹åŒ–å¤±è´¥: {str(e)}"

    def generate_single_image(self, slot_index: int, custom_prompt: str, reference_image, 
                             ip_adapter_path: str, base_model: str, image_encoder_path: str, 
                             image_encoder_2_path: str, use_offload: bool, steps: int, 
                             guidance_scale: float, subject_scale: float) -> Tuple[str, Any]:
        """ç”Ÿæˆå•å¼ å›¾ç‰‡"""
        try:
            if not self.current_project_dir:
                return "âŒ è¯·å…ˆç”Ÿæˆå†…å®¹", None
            
            if reference_image is None:
                return "âŒ è¯·ä¸Šä¼ è§’è‰²ä¸€è‡´æ€§å‚è€ƒå›¾", None
            
            # ä½¿ç”¨è‡ªå®šä¹‰æç¤ºè¯æˆ–é»˜è®¤æç¤ºè¯
            if slot_index < len(self.current_prompts):
                prompt = custom_prompt.strip() if custom_prompt.strip() else self.current_prompts[slot_index]
            else:
                prompt = custom_prompt.strip() if custom_prompt.strip() else f"Anime style scene {slot_index + 1}"
            
            # ä¿å­˜å‚è€ƒå›¾åˆ°é¡¹ç›®ç›®å½•
            ref_image_path = self.current_project_dir / "reference_image.png"
            reference_image.save(ref_image_path)
            
            # åˆå§‹åŒ–å›¾ç‰‡ç”Ÿæˆå™¨ï¼ˆå¦‚æœæœªåˆå§‹åŒ–ï¼‰
            if not self.image_generator:
                success, message = self.initialize_image_generator(
                    ip_adapter_path, base_model, image_encoder_path, 
                    image_encoder_2_path, use_offload
                )
                if not success:
                    return f"âŒ {message}", None
            
            # åˆ›å»ºå›¾ç‰‡è¾“å‡ºç›®å½•
            images_dir = self.current_project_dir / "generated_images"
            images_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå•å¼ å›¾ç‰‡
            output_path = images_dir / f"image_{slot_index:03d}.png"
            
            # è°ƒç”¨å›¾ç‰‡ç”Ÿæˆå™¨
            try:
                generated_path = self.image_generator.generate(
                    prompt=prompt,
                    reference_image=str(ref_image_path),
                    output_path=str(output_path),
                    steps=steps,
                    guidance_scale=guidance_scale,
                    subject_scale=subject_scale,
                    seed=random.randint(1000, 999999)
                )
                    
            except TypeError:
                # å¦‚æœå‚æ•°ä¸å¯¹ï¼Œå°è¯•æ›´ç®€å•çš„è°ƒç”¨
                try:
                    generated_path = self.image_generator.generate(
                        prompt, str(ref_image_path), str(output_path)
                    )
                except Exception as e:
                    return f"âŒ å›¾ç‰‡ç”Ÿæˆè°ƒç”¨å¤±è´¥: {str(e)}", None
            
            if generated_path and Path(generated_path).exists():
                # è¯»å–ç”Ÿæˆçš„å›¾ç‰‡
                img = Image.open(generated_path)
                # æ›´æ–°å›¾ç‰‡æ§½çŠ¶æ€
                if slot_index < len(self.current_images):
                    self.current_images[slot_index] = str(generated_path)
                return f"âœ… é‡æ–°ç”Ÿæˆç¬¬ {slot_index + 1} å¼ å›¾ç‰‡æˆåŠŸ", img
            else:
                return f"âŒ é‡æ–°ç”Ÿæˆç¬¬ {slot_index + 1} å¼ å›¾ç‰‡å¤±è´¥", None
                
        except Exception as e:
            return f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{str(e)}", None

    def batch_generate_images_direct(self, reference_image, ip_adapter_path: str, base_model: str, 
                                    image_encoder_path: str, image_encoder_2_path: str, use_offload: bool, 
                                    steps: int, guidance_scale: float, subject_scale: float, 
                                    progress=gr.Progress()):
        """ç›´æ¥æ‰¹é‡ç”Ÿæˆå›¾ç‰‡ï¼Œè¿”å›PIL Imageå¯¹è±¡"""
        try:
            if not self.current_project_dir:
                return ["âŒ è¯·å…ˆç”Ÿæˆå†…å®¹"] + [None] * 10
            
            if not self.current_prompts:
                return ["âŒ æ²¡æœ‰æ‰¾åˆ°æç¤ºè¯ï¼Œè¯·å…ˆç”Ÿæˆå†…å®¹"] + [None] * 10
            
            if reference_image is None:
                return ["âŒ è¯·ä¸Šä¼ è§’è‰²ä¸€è‡´æ€§å‚è€ƒå›¾"] + [None] * 10
            
            # ä¿å­˜å‚è€ƒå›¾
            ref_image_path = self.current_project_dir / "reference_image.png"
            reference_image.save(ref_image_path)
            
            # åˆå§‹åŒ–ç”Ÿæˆå™¨
            progress(0, desc="ğŸ”§ æ­£åœ¨åˆå§‹åŒ–å›¾ç‰‡ç”Ÿæˆå™¨...")
            success, message = self.initialize_image_generator(
                ip_adapter_path, base_model, image_encoder_path, 
                image_encoder_2_path, use_offload
            )
            if not success:
                return [f"âŒ {message}"] + [None] * 10
            
            # åˆ›å»ºè¾“å‡ºç›®å½•
            images_dir = self.current_project_dir / "generated_images"
            images_dir.mkdir(exist_ok=True)
            
            # ç”Ÿæˆå›¾ç‰‡
            slot_images = [None] * 10
            generated_count = 0
            total_prompts = len(self.current_prompts)
            
            for i, prompt in enumerate(self.current_prompts):
                progress((i + 1) / total_prompts, desc=f"ğŸ¨ æ­£åœ¨ç”Ÿæˆç¬¬ {i + 1}/{total_prompts} å¼ å›¾ç‰‡...")
                
                output_path = images_dir / f"image_{i:03d}.png"
                
                try:
                    generated_path = self.image_generator.generate(
                        prompt=prompt,
                        reference_image=str(ref_image_path),
                        output_path=str(output_path),
                        steps=steps,
                        guidance_scale=guidance_scale,
                        subject_scale=subject_scale,
                        seed=random.randint(1000, 999999)
                    )
                    
                    if generated_path and Path(generated_path).exists():
                        # è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸ºPIL Imageå¯¹è±¡
                        img = Image.open(generated_path)
                        
                        if i < 10:
                            slot_images[i] = img  # ç›´æ¥ä¼ é€’PIL Imageå¯¹è±¡
                        
                        generated_count += 1
                        print(f"âœ… ç¬¬ {i + 1} å¼ å›¾ç‰‡ç”ŸæˆæˆåŠŸ: {generated_path}")
                    else:
                        print(f"âŒ ç¬¬ {i + 1} å¼ å›¾ç‰‡ç”Ÿæˆå¤±è´¥")
                        
                except Exception as e:
                    print(f"ç”Ÿæˆç¬¬ {i + 1} å¼ å›¾ç‰‡æ—¶å‡ºé”™: {e}")
                    continue
            
            progress(1.0, desc="ğŸ‰ å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼")
            status = f"ğŸ‰ å›¾ç‰‡ç”Ÿæˆå®Œæˆï¼æˆåŠŸç”Ÿæˆ {generated_count}/{total_prompts} å¼ å›¾ç‰‡"
            return [status] + slot_images
            
        except Exception as e:
            print(f"æ‰¹é‡ç”Ÿæˆå‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
            return [f"âŒ ç”Ÿæˆå¤±è´¥ï¼š{str(e)}"] + [None] * 10

    def validate_images_with_vlm(self, api_key: str, base_url: str, model_name: str, 
                                progress=gr.Progress()) -> Tuple[str, List[Dict]]:
        """ä½¿ç”¨è§†è§‰è¯­è¨€æ¨¡å‹éªŒè¯å›¾ç‰‡"""
        try:
            if not self.current_project_dir:
                return "âŒ æ²¡æœ‰å½“å‰é¡¹ç›®ï¼Œè¯·å…ˆç”Ÿæˆå†…å®¹å’Œå›¾ç‰‡", []
            
            images_dir = self.current_project_dir / "generated_images"
            if not images_dir.exists():
                return "âŒ æ‰¾ä¸åˆ°ç”Ÿæˆçš„å›¾ç‰‡ï¼Œè¯·å…ˆç”Ÿæˆå›¾ç‰‡", []
            
            if not all([api_key, model_name]):
                return "âŒ è¯·å¡«å†™å®Œæ•´çš„VLM APIé…ç½®", []
            
            progress(0.1, "æ­£åœ¨åˆå§‹åŒ–è§†è§‰è¯­è¨€æ¨¡å‹...")
            
            # åˆå§‹åŒ–VLMéªŒè¯å™¨
            self.vlm_validator = VLMValidator(
                api_key=api_key,
                base_url=base_url,
                model=model_name
            )
            
            progress(0.2, "å¼€å§‹éªŒè¯å›¾ç‰‡...")
            
            # è·å–å›¾ç‰‡æ–‡ä»¶åˆ—è¡¨
            image_files = sorted(list(images_dir.glob("*.png")) + list(images_dir.glob("*.jpg")))
            
            validation_results = []
            
            for i, image_path in enumerate(image_files):
                progress(0.2 + (i + 1) / len(image_files) * 0.7, f"æ­£åœ¨éªŒè¯ç¬¬ {i + 1}/{len(image_files)} å¼ å›¾ç‰‡...")
                
                try:
                    # è·å–å¯¹åº”çš„æç¤ºè¯
                    prompt = self.current_prompts[i] if i < len(self.current_prompts) else "Unknown prompt"
                    
                    # éªŒè¯å•å¼ å›¾ç‰‡
                    result = self.vlm_validator.validate_single_image(
                        image_path=str(image_path),
                        original_prompt=prompt
                    )
                    
                    if result["success"]:
                        validation_data = {
                            "index": i + 1,
                            "image_path": str(image_path),
                            "original_prompt": prompt,
                            "analysis": result["analysis"],
                            "score": result.get("score", 0),
                            "suggestions": result.get("suggestions", ""),
                            "compliance": result.get("compliance", "unknown"),
                            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                    else:
                        validation_data = {
                            "index": i + 1,
                            "image_path": str(image_path),
                            "original_prompt": prompt,
                            "analysis": f"éªŒè¯å¤±è´¥: {result.get('error', 'æœªçŸ¥é”™è¯¯')}",
                            "score": 0,
                            "suggestions": "è¯·é‡æ–°ç”Ÿæˆå›¾ç‰‡",
                            "compliance": "error",
                            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                        }
                    
                    validation_results.append(validation_data)
                    
                except Exception as e:
                    validation_data = {
                        "index": i + 1,
                        "image_path": str(image_path),
                        "original_prompt": prompt,
                        "analysis": f"éªŒè¯å‡ºé”™: {str(e)}",
                        "score": 0,
                        "suggestions": "è¯·æ£€æŸ¥é…ç½®é‡æ–°éªŒè¯",
                        "compliance": "error",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                    }
                    validation_results.append(validation_data)
            
            # ä¿å­˜éªŒè¯ç»“æœ
            results_file = self.current_project_dir / "validation_results.json"
            with open(results_file, 'w', encoding='utf-8') as f:
                json.dump(validation_results, f, ensure_ascii=False, indent=2)
            
            progress(1.0, "éªŒè¯å®Œæˆï¼")
            
            # ç”ŸæˆçŠ¶æ€æŠ¥å‘Š
            successful_count = len([r for r in validation_results if r["compliance"] not in ["error", "unknown"]])
            status_report = f"âœ… å›¾ç‰‡éªŒè¯å®Œæˆï¼éªŒè¯äº† {len(validation_results)} å¼ å›¾ç‰‡ï¼Œ{successful_count} å¼ æˆåŠŸ"
            
            return status_report, validation_results
                
        except Exception as e:
            return f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {str(e)}", []

# åˆ›å»ºå…¨å±€pipelineå®ä¾‹
pipeline = VideoGenerationPipeline()

def create_interface():
    """åˆ›å»ºGradioç•Œé¢"""
    
    # è®¾ç½®Gradioä¸´æ—¶ç›®å½•ä¸ºå½“å‰ç›®å½•ä¸‹çš„tmp
    temp_dir = Path("./tmp")
    temp_dir.mkdir(exist_ok=True)
    os.environ["GRADIO_TEMP_DIR"] = str(temp_dir.absolute())
    
    with gr.Blocks(title="ğŸ¬ AIè§†é¢‘ç”Ÿæˆæ’ç‰ˆå·¥å…·", theme=gr.themes.Soft()) as app:
        gr.Markdown("""
        # ğŸ¬ AIè§†é¢‘ç”Ÿæˆæ’ç‰ˆå·¥å…·
        
        **å®Œæ•´çš„è§†é¢‘å†…å®¹ç”Ÿæˆpipelineï¼šæœç´¢è¯ â†’ å†…å®¹ç”Ÿæˆ â†’ å›¾ç‰‡ç”Ÿæˆ â†’ å¤šæ¨¡æ€éªŒè¯**
        """)
        
        # çŠ¶æ€å˜é‡
        current_scene_count = gr.State(5)
        
        with gr.Tab("ğŸ“ å†…å®¹ç”Ÿæˆ"):
            gr.Markdown("### ğŸ”§ åŸºç¡€é…ç½®")
            
            with gr.Row():
                with gr.Column(scale=2):
                    search_query = gr.Textbox(
                        label="æœç´¢è¯",
                        placeholder="è¯·è¾“å…¥ä¸»é¢˜ï¼Œå¦‚ï¼šæ˜¥å¤©åœ¨å“ªé‡Œã€å¡çš®å·´æ‹‰çš„ä¸€å¤©"
                    )
                    
                with gr.Column(scale=1):
                    scene_count = gr.Slider(
                        minimum=3,
                        maximum=10,
                        value=5,
                        step=1,
                        label="åœºæ™¯æ•°é‡"
                    )
            
            gr.Markdown("### ğŸ¤– è¯­è¨€æ¨¡å‹é…ç½®")
            
            with gr.Row():
                llm_api_key = gr.Textbox(
                    label="API Key",
                    type="password",
                    placeholder="sk-..."
                )
                
                llm_base_url = gr.Textbox(
                    label="Base URL",
                    placeholder="https://api.openai.com/v1"
                )
                
                llm_model = gr.Textbox(
                    label="æ¨¡å‹åç§°",
                    value="deepseek-v3-250324",
                    placeholder="gpt-4o-mini, deepseek-v3ç­‰"
                )
            
            with gr.Row():
                generate_btn = gr.Button("ğŸš€ ç”Ÿæˆå†…å®¹", variant="primary", size="lg")
                save_btn = gr.Button("ğŸ’¾ ä¿å­˜ç¼–è¾‘", variant="secondary", size="lg")
            
            gr.Markdown("### ğŸ“Š ç”Ÿæˆç»“æœ")
            
            with gr.Row():
                generation_status = gr.Textbox(
                    label="ç”ŸæˆçŠ¶æ€",
                    lines=3,
                    interactive=False,
                    show_copy_button=True
                )
            
            # äº”ç§ç»“æœçš„æ˜¾ç¤ºå’Œç¼–è¾‘åŒºåŸŸ
            with gr.Tab("ğŸ–¼ï¸ å›¾ç‰‡ç”Ÿæˆæç¤ºè¯"):
                img_prompts = gr.Textbox(
                    label="å›¾ç‰‡ç”Ÿæˆæç¤ºè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                    lines=15,
                    placeholder="Anime style, high quality..."
                )
            
            with gr.Tab("ğŸ¥ è§†é¢‘ç”Ÿæˆæç¤ºè¯"):
                vid_prompts = gr.Textbox(
                    label="è§†é¢‘ç”Ÿæˆæç¤ºè¯ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                    lines=15,
                    placeholder="Anime style, slow push-in shot..."
                )
            
            with gr.Tab("ğŸ™ï¸ æ—ç™½æ–‡æœ¬"):
                narrations = gr.Textbox(
                    label="æ—ç™½æ–‡æœ¬ï¼ˆæ¯è¡Œä¸€ä¸ªï¼‰",
                    lines=10,
                    placeholder="æ—ç™½1\næ—ç™½2\n..."
                )
            
            with gr.Tab("ğŸ’¼ ä¸šåŠ¡ç‚¹"):
                business_points = gr.Textbox(
                    label="è§†é¢‘ä¸šåŠ¡ç‚¹",
                    lines=5,
                    placeholder="3-5ä¸ªæ ¸å¿ƒä»·å€¼ç‚¹..."
                )
            
            with gr.Tab("ğŸ“‹ æœåŠ¡æ¦‚è¿°"):
                service_overview = gr.Textbox(
                    label="æœåŠ¡æ¦‚è¿°",
                    lines=5,
                    placeholder="100å­—ä»¥å†…çš„æè¿°..."
                )
            
            # é‡æ–°ç”ŸæˆåŒºåŸŸ
            gr.Markdown("### ğŸ”„ é‡æ–°ç”Ÿæˆ")
            
            with gr.Row():
                dissatisfaction = gr.Textbox(
                    label="ä¸æ»¡æ„çš„ç‚¹",
                    placeholder="è¯·æè¿°å½“å‰ç»“æœçš„ä¸æ»¡æ„ä¹‹å¤„ï¼Œå¦‚ï¼šæ•…äº‹ä¸å¤Ÿè¿è´¯ã€è§’è‰²æè¿°ä¸å¤Ÿè¯¦ç»†ç­‰"
                )
                
                regenerate_btn = gr.Button("ğŸ”„ é‡æ–°ç”Ÿæˆ", variant="secondary")
        
        with gr.Tab("ğŸ¨ å›¾ç‰‡ç”Ÿæˆ"):
            gr.Markdown("### ğŸ–¼ï¸ è§’è‰²ä¸€è‡´æ€§é…ç½®")
            
            with gr.Row():
                with gr.Column(scale=1):
                    reference_image = gr.Image(
                        label="è§’è‰²ä¸€è‡´æ€§å‚è€ƒå›¾ - ä¸Šä¼ ä¸€å¼ è§’è‰²å‚è€ƒå›¾ï¼Œç¡®ä¿ç”Ÿæˆå›¾ç‰‡çš„è§’è‰²ä¸€è‡´æ€§",
                        type="pil"
                    )
                    
                with gr.Column(scale=2):
                    gr.Markdown("### âš™ï¸ æ¨¡å‹é…ç½®")
                    
                    ip_adapter_path = gr.Textbox(
                        label="IP Adapterè·¯å¾„",
                        value="/data/home/lizhijun/llm/flux-hf/InstantCharacter-main/tencent/InstantCharacter/instantcharacter_ip-adapter.bin"
                    )
                    
                    base_model = gr.Textbox(
                        label="åŸºç¡€æ¨¡å‹è·¯å¾„",
                        value="/data/home/lizhijun/llm/flux-hf/model/flux-dev"
                    )
                    
                    with gr.Row():
                        image_encoder_path = gr.Textbox(
                            label="å›¾åƒç¼–ç å™¨1",
                            value="google/siglip-so400m-patch14-384"
                        )
                        
                        image_encoder_2_path = gr.Textbox(
                            label="å›¾åƒç¼–ç å™¨2", 
                            value="facebook/dinov2-giant"
                        )
            
            gr.Markdown("### ğŸ”§ ç”Ÿæˆå‚æ•°")
            
            with gr.Row():
                use_offload = gr.Checkbox(
                    label="å¯ç”¨æ¨¡å‹å¸è½½ - å¯ç”¨åä½¿ç”¨CPUå¸è½½èŠ‚çœæ˜¾å­˜ï¼Œä½†é€Ÿåº¦è¾ƒæ…¢",
                    value=True
                )
                
                steps = gr.Slider(
                    minimum=10,
                    maximum=50,
                    value=28,
                    step=1,
                    label="æ¨ç†æ­¥æ•° - æ›´å¤šæ­¥æ•°é€šå¸¸è´¨é‡æ›´å¥½ä½†é€Ÿåº¦æ›´æ…¢"
                )
            
            with gr.Row():
                guidance_scale = gr.Slider(
                    minimum=1.0,
                    maximum=10.0,
                    value=3.5,
                    step=0.1,
                    label="å¼•å¯¼å¼ºåº¦ - æ§åˆ¶ç”Ÿæˆå›¾ç‰‡å¯¹æç¤ºè¯çš„éµå¾ªç¨‹åº¦"
                )
                
                subject_scale = gr.Slider(
                    minimum=0.1,
                    maximum=1.0,
                    value=0.9,
                    step=0.1,
                    label="è§’è‰²å¼ºåº¦ - æ§åˆ¶å‚è€ƒå›¾ç‰‡è§’è‰²ç‰¹å¾çš„ä¿æŒç¨‹åº¦"
                )
            
            gr.Markdown("### ğŸš€ å›¾ç‰‡ç”Ÿæˆ")
            
            with gr.Row():
                batch_generate_btn = gr.Button("ğŸ¨ æ‰¹é‡ç”Ÿæˆæ‰€æœ‰å›¾ç‰‡", variant="primary", size="lg")
            
            with gr.Row():
                image_generation_status = gr.Textbox(
                    label="å›¾ç‰‡ç”ŸæˆçŠ¶æ€",
                    lines=2,
                    interactive=False
                )
            
            # åŠ¨æ€å›¾ç‰‡ç”Ÿæˆæ§½ä½
            gr.Markdown("### ğŸ–¼ï¸ å›¾ç‰‡ç”Ÿæˆæ§½ä½")
            
            # åˆ›å»ºå›¾ç‰‡æ§½ä½ç»„ä»¶
            image_slot_components = []
            for i in range(10):
                with gr.Row(visible=False) as slot_row:
                    with gr.Column(scale=3):
                        slot_prompt = gr.Textbox(
                            label=f"åœºæ™¯ {i+1} æç¤ºè¯",
                            lines=2,
                            placeholder="å¯ä»¥ä¿®æ”¹æç¤ºè¯åé‡æ–°ç”Ÿæˆå•å¼ å›¾ç‰‡"
                        )
                    
                    with gr.Column(scale=2):
                        slot_image = gr.Image(
                            label=f"åœºæ™¯ {i+1}",
                            type="pil",
                            height=200
                        )
                    
                    with gr.Column(scale=1):
                        slot_generate_btn = gr.Button(
                            f"ğŸ¨ é‡æ–°ç”Ÿæˆç¬¬{i+1}å¼ ",
                            variant="secondary",
                            size="lg"  # å¢å¤§æŒ‰é’®
                        )
                        
                        slot_status = gr.Textbox(
                            label="çŠ¶æ€",
                            lines=1,
                            interactive=False,
                            value="â³ ç­‰å¾…ç”Ÿæˆ"  # é»˜è®¤çŠ¶æ€
                        )
                
                image_slot_components.append({
                    "index": i,
                    "row": slot_row,
                    "prompt": slot_prompt,
                    "image": slot_image,
                    "button": slot_generate_btn,
                    "status": slot_status
                })
        
        with gr.Tab("ğŸ” å¤šæ¨¡æ€éªŒè¯"):
            gr.Markdown("### ğŸ¤– è§†è§‰è¯­è¨€æ¨¡å‹é…ç½®")
            
            with gr.Row():
                vlm_api_key = gr.Textbox(
                    label="VLM API Key",
                    type="password",
                    placeholder="sk-..."
                )
                
                vlm_base_url = gr.Textbox(
                    label="VLM Base URL",
                    placeholder="https://api.openai.com/v1"
                )
                
                vlm_model = gr.Textbox(
                    label="VLMæ¨¡å‹åç§°",
                    value="gpt-4o-mini",
                    placeholder="gpt-4o-mini, claude-3-sonnetç­‰"
                )
            
            with gr.Row():
                validate_btn = gr.Button("ğŸ” å¼€å§‹éªŒè¯", variant="primary", size="lg")
            
            validation_status = gr.Textbox(
                label="éªŒè¯çŠ¶æ€",
                lines=3,
                interactive=False
            )
            
            # éªŒè¯ç»“æœå¯è§†åŒ–
            gr.Markdown("### ğŸ“Š éªŒè¯ç»“æœè¯¦æƒ…")
            
            validation_results_display = gr.JSON(
                label="è¯¦ç»†éªŒè¯ç»“æœ",
                visible=True
            )
        
        with gr.Tab("ğŸ“Š é¡¹ç›®ç®¡ç†"):
            gr.Markdown("### ğŸ“ å½“å‰é¡¹ç›®çŠ¶æ€")
            
            project_info = gr.Textbox(
                label="é¡¹ç›®ä¿¡æ¯",
                lines=10,
                interactive=False
            )
            
            with gr.Row():
                refresh_project_btn = gr.Button("ğŸ”„ åˆ·æ–°é¡¹ç›®ä¿¡æ¯", variant="secondary")
                export_project_btn = gr.Button("ğŸ“¦ å¯¼å‡ºé¡¹ç›®", variant="primary")
        
        # äº‹ä»¶ç»‘å®š
        def update_scene_count(count):
            current_scene_count.value = count
            return count
        
        def update_image_slots_visibility(count):
            """æ ¹æ®åœºæ™¯æ•°é‡æ˜¾ç¤ºå¯¹åº”æ•°é‡çš„å›¾ç‰‡æ§½ä½"""
            print(f"æ›´æ–°æ§½ä½å¯è§æ€§ï¼Œåœºæ™¯æ•°é‡: {count}")  # è°ƒè¯•è¾“å‡º
            updates = []
            for i in range(10):  # åªå¤„ç†10ä¸ªæ§½ä½
                if i < count:
                    updates.append(gr.update(visible=True))
                    print(f"æ§½ä½ {i+1}: å¯è§")  # è°ƒè¯•è¾“å‡º
                else:
                    updates.append(gr.update(visible=False))
                    print(f"æ§½ä½ {i+1}: éšè—")  # è°ƒè¯•è¾“å‡º
            return updates
        
        def update_slots_with_prompts():
            """ç”¨ç”Ÿæˆçš„æç¤ºè¯æ›´æ–°æ§½ä½"""
            if not pipeline.current_prompts:
                return [gr.update() for _ in range(10)]
            
            updates = []
            for i in range(10):
                if i < len(pipeline.current_prompts):
                    updates.append(gr.update(value=pipeline.current_prompts[i]))
                else:
                    updates.append(gr.update())
            return updates
        
        def reset_slot_status():
            """é‡ç½®æ‰€æœ‰æ§½ä½çŠ¶æ€ä¸ºç­‰å¾…ç”Ÿæˆ"""
            status_updates = []
            for i in range(10):
                if i < len(pipeline.current_prompts):
                    status_updates.append(gr.update(value="â³ ç­‰å¾…ç”Ÿæˆ"))
                else:
                    status_updates.append(gr.update())
            return status_updates
        
        def update_slot_status_batch_complete():
            """æ‰¹é‡ç”Ÿæˆå®Œæˆåæ›´æ–°æ§½ä½çŠ¶æ€"""
            status_updates = []
            for i in range(10):
                if i < len(pipeline.current_prompts):
                    status_updates.append(gr.update(value="âœ… ç”Ÿæˆå®Œæˆ"))
                else:
                    status_updates.append(gr.update())
            return status_updates
        
        scene_count.change(
            fn=update_scene_count, 
            inputs=[scene_count], 
            outputs=[current_scene_count]
        )
        
        # ç”Ÿæˆå†…å®¹
        generate_btn.click(
            fn=pipeline.generate_content,
            inputs=[search_query, llm_api_key, llm_base_url, llm_model, scene_count],
            outputs=[generation_status, img_prompts, vid_prompts, narrations, business_points, service_overview]
        ).then(
            fn=update_slots_with_prompts,
            outputs=[slot["prompt"] for slot in image_slot_components]
        ).then(
            fn=update_image_slots_visibility,
            inputs=[scene_count],
            outputs=[slot["row"] for slot in image_slot_components]
        ).then(
            fn=reset_slot_status,
            outputs=[slot["status"] for slot in image_slot_components]
        )
        
        # ä¿å­˜ç¼–è¾‘
        save_btn.click(
            fn=pipeline.save_edited_content,
            inputs=[img_prompts, vid_prompts, narrations, business_points, service_overview],
            outputs=[generation_status]
        ).then(
            fn=update_slots_with_prompts,
            outputs=[slot["prompt"] for slot in image_slot_components]
        ).then(
            fn=lambda prompts_text: len([p for p in prompts_text.split('\n') if p.strip()]),
            inputs=[img_prompts],
            outputs=[current_scene_count]
        ).then(
            fn=update_image_slots_visibility,
            inputs=[current_scene_count],
            outputs=[slot["row"] for slot in image_slot_components]
        ).then(
            fn=reset_slot_status,
            outputs=[slot["status"] for slot in image_slot_components]
        )
        
        # é‡æ–°ç”Ÿæˆ
        regenerate_btn.click(
            fn=pipeline.regenerate_content,
            inputs=[search_query, llm_api_key, llm_base_url, llm_model, scene_count, dissatisfaction],
            outputs=[generation_status, img_prompts, vid_prompts, narrations, business_points, service_overview]
        ).then(
            fn=update_slots_with_prompts,
            outputs=[slot["prompt"] for slot in image_slot_components]
        ).then(
            fn=update_image_slots_visibility,
            inputs=[scene_count],
            outputs=[slot["row"] for slot in image_slot_components]
        ).then(
            fn=reset_slot_status,
            outputs=[slot["status"] for slot in image_slot_components]
        )
        
        # æ‰¹é‡ç”Ÿæˆå›¾ç‰‡
        batch_generate_btn.click(
            fn=pipeline.batch_generate_images_direct,
            inputs=[reference_image, ip_adapter_path, base_model, image_encoder_path, 
                   image_encoder_2_path, use_offload, steps, guidance_scale, subject_scale],
            outputs=[image_generation_status] + [slot["image"] for slot in image_slot_components],
            show_progress=True
        ).then(
            fn=update_slot_status_batch_complete,
            outputs=[slot["status"] for slot in image_slot_components]
        )
        
        # å•å¼ å›¾ç‰‡ç”Ÿæˆ
        def create_single_generate_function(slot_index):
            """ä¸ºæ¯ä¸ªæ§½ä½åˆ›å»ºå•ç‹¬çš„ç”Ÿæˆå‡½æ•°"""
            def single_generate(custom_prompt, ref_img, ip_path, base_mdl, enc_path1, enc_path2, 
                               offload, step_count, guide_scale, subj_scale):
                return pipeline.generate_single_image(
                    slot_index, custom_prompt, ref_img, ip_path, base_mdl, 
                    enc_path1, enc_path2, offload, step_count, guide_scale, subj_scale
                )
            return single_generate
        
        # ä¸ºæ¯ä¸ªæ§½ä½ç»‘å®šå•ç‹¬çš„ç”Ÿæˆå‡½æ•°
        for slot in image_slot_components:
            slot_index = slot["index"]
            single_gen_func = create_single_generate_function(slot_index)
            
            slot["button"].click(
                fn=single_gen_func,
                inputs=[
                    slot["prompt"], reference_image, ip_adapter_path, base_model,
                    image_encoder_path, image_encoder_2_path, use_offload, steps,
                    guidance_scale, subject_scale
                ],
                outputs=[slot["status"], slot["image"]]
            )
        
        # éªŒè¯å›¾ç‰‡
        validate_btn.click(
            fn=pipeline.validate_images_with_vlm,
            inputs=[vlm_api_key, vlm_base_url, vlm_model],
            outputs=[validation_status, validation_results_display],
            show_progress=True
        )
        
        # åˆ·æ–°é¡¹ç›®ä¿¡æ¯
        def refresh_project_info():
            if pipeline.current_project_dir and pipeline.current_project_dir.exists():
                info_lines = [
                    f"ğŸ“ é¡¹ç›®ç›®å½•: {pipeline.current_project_dir}",
                    f"ğŸ“… åˆ›å»ºæ—¶é—´: {datetime.fromtimestamp(pipeline.current_project_dir.stat().st_ctime)}",
                    "",
                    "ğŸ“‹ é¡¹ç›®æ–‡ä»¶:",
                ]
                
                for file_path in sorted(pipeline.current_project_dir.glob("*")):
                    if file_path.is_file():
                        size = file_path.stat().st_size
                        info_lines.append(f"  ğŸ“„ {file_path.name} ({size} bytes)")
                    elif file_path.is_dir():
                        file_count = len(list(file_path.glob("*")))
                        info_lines.append(f"  ğŸ“ {file_path.name}/ ({file_count} files)")
                
                return "\n".join(info_lines)
            else:
                return "âŒ æ²¡æœ‰å½“å‰é¡¹ç›®"
        
        refresh_project_btn.click(
            fn=refresh_project_info,
            outputs=[project_info]
        )
        
        # å¯¼å‡ºé¡¹ç›®
        def export_project():
            if not pipeline.current_project_dir or not pipeline.current_project_dir.exists():
                return "âŒ æ²¡æœ‰å¯å¯¼å‡ºçš„é¡¹ç›®"
            
            try:
                # åˆ›å»ºå¯¼å‡ºç›®å½•
                export_dir = Path("exports")
                export_dir.mkdir(exist_ok=True)
                
                # åˆ›å»ºzipæ–‡ä»¶
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                export_filename = export_dir / f"{pipeline.current_project_dir.name}_{timestamp}.zip"
                
                import zipfile
                with zipfile.ZipFile(export_filename, 'w', zipfile.ZIP_DEFLATED) as zipf:
                    for file_path in pipeline.current_project_dir.rglob('*'):
                        if file_path.is_file():
                            arcname = file_path.relative_to(pipeline.current_project_dir)
                            zipf.write(file_path, arcname)
                
                return f"âœ… é¡¹ç›®å·²å¯¼å‡ºåˆ°: {export_filename}"
                
            except Exception as e:
                return f"âŒ å¯¼å‡ºå¤±è´¥: {str(e)}"
        
        export_project_btn.click(
            fn=export_project,
            outputs=[generation_status]
        )
    
    return app

if __name__ == "__main__":
    print("ğŸš€ æ­£åœ¨å¯åŠ¨AIè§†é¢‘ç”Ÿæˆæ’ç‰ˆå·¥å…·...")
    print("ğŸ“ ç«¯å£: 7861")
    
    try:
        # åˆ›å»ºæœ¬åœ°ä¸´æ—¶ç›®å½•
        temp_dir = Path("./tmp")
        temp_dir.mkdir(exist_ok=True)
        print(f"âœ… ä¸´æ—¶ç›®å½•è®¾ç½®: {temp_dir.absolute()}")
        
        app = create_interface()
        print("âœ… ç•Œé¢åˆ›å»ºæˆåŠŸ")
        
        # æ£€æŸ¥ç«¯å£æ˜¯å¦å¯ç”¨
        import socket
        def is_port_available(port):
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                try:
                    s.bind(('', port))
                    return True
                except socket.error:
                    return False
        
        port = 7861
        if not is_port_available(port):
            print(f"âš ï¸  ç«¯å£ {port} è¢«å ç”¨ï¼Œå°è¯•å…¶ä»–ç«¯å£...")
            for new_port in range(7862, 7870):
                if is_port_available(new_port):
                    port = new_port
                    print(f"âœ… ä½¿ç”¨ç«¯å£: {port}")
                    break
            else:
                print("âŒ æ‰¾ä¸åˆ°å¯ç”¨ç«¯å£")
                exit(1)
        
        print(f"ğŸŒ å¯åŠ¨æœåŠ¡å™¨: http://192.168.99.119:{port}")
        
        # ä½¿ç”¨æœ€ç®€å•çš„å¯åŠ¨æ–¹å¼é¿å…æƒé™é—®é¢˜
        app.launch(
            server_name="192.168.99.119",
            server_port=port,
            share=False
        )
        
    except Exception as e:
        print(f"âŒ å¯åŠ¨å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        
        print("\nğŸ”§ æ•…éšœæ’é™¤å»ºè®®ï¼š")
        print("1. æ£€æŸ¥ç«¯å£7861æ˜¯å¦è¢«å ç”¨")
        print("2. ç¡®è®¤srcç›®å½•å’Œç›¸å…³æ¨¡å—å­˜åœ¨")
        print("3. æ£€æŸ¥Pythonä¾èµ–æ˜¯å¦å®Œæ•´")
        print("4. æ£€æŸ¥Gradioç‰ˆæœ¬å…¼å®¹æ€§")
        print("5. å°è¯•é‡æ–°å®‰è£…Gradio: pip install --upgrade gradio")
