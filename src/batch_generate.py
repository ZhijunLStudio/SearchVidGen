# Copyright 2024-2025 The Alibaba Wan Team Authors. All rights reserved.
# MODIFIED FOR BATCH PROCESSING
import argparse
import logging
import os
import sys
import warnings
from pathlib import Path
from PIL import Image
import glob
import torch
import torch.distributed as dist


import wan
from wan.configs import SIZE_CONFIGS, WAN_CONFIGS, MAX_AREA_CONFIGS
from wan.utils.utils import cache_video, str2bool

warnings.filterwarnings('ignore')

def _init_logging(rank):
    """åˆå§‹åŒ–æ—¥å¿—è®°å½•å™¨"""
    if rank == 0:
        logging.basicConfig(
            level=logging.INFO,
            format="[%(asctime)s] %(levelname)s: %(message)s",
            handlers=[logging.StreamHandler(stream=sys.stdout)])
    else:
        logging.basicConfig(level=logging.ERROR)

def _parse_args():
    """è§£æå‘½ä»¤è¡Œå‚æ•°"""
    parser = argparse.ArgumentParser(description="Batch generate videos from images and prompts using Wan-AI.")
    
    # --- æ ¸å¿ƒè¾“å…¥å‚æ•° ---
    parser.add_argument("--project_dir", type=str, required=True, help="Path to the project directory containing 'generated_images' and 'img2vid_prompts.txt'.")
    parser.add_argument("--output_dir", type=str, required=True, help="Directory to save the generated video clips.")
    parser.add_argument("--ckpt_dir", type=str, required=True, help="The path to the Wan-AI model checkpoint directory.")

    # --- æ¨¡å‹å’Œç”Ÿæˆå‚æ•° ---
    parser.add_argument("--size", type=str, default="832*480", help="Resolution of the generated videos.")
    parser.add_argument("--frame_num", type=int, default=81, help="Number of frames to generate for each video clip.")
    parser.add_argument("--sample_steps", type=int, default=40, help="The sampling steps.")
    parser.add_argument("--sample_guide_scale", type=float, default=6.0, help="Classifier free guidance scale.")
    parser.add_argument("--sample_shift", type=float, default=3.0, help="Sampling shift factor for flow matching schedulers.")
    parser.add_argument("--base_seed", type=int, default=42, help="Base seed for reproducibility.")

    # --- æ€§èƒ½å’Œåˆ†å¸ƒå¼è®¡ç®—å‚æ•° ---
    parser.add_argument("--offload_model", type=str2bool, default=None, help="Offload model to CPU to save GPU memory.")
    parser.add_argument("--ulysses_size", type=int, default=1, help="Ulysses parallelism size.")
    parser.add_argument("--ring_size", type=int, default=1, help="Ring attention parallelism size.")
    parser.add_argument("--t5_fsdp", action="store_true", default=False, help="Use FSDP for T5.")
    parser.add_argument("--t5_cpu", action="store_true", default=False, help="Place T5 model on CPU.")
    parser.add_argument("--dit_fsdp", action="store_true", default=False, help="Use FSDP for DiT.")

    return parser.parse_args()


def main():
    args = _parse_args()
    
    # 1. åˆå§‹åŒ–åˆ†å¸ƒå¼ç¯å¢ƒ
    rank = int(os.getenv("RANK", 0))
    world_size = int(os.getenv("WORLD_SIZE", 1))
    local_rank = int(os.getenv("LOCAL_RANK", 0))
    device = local_rank
    _init_logging(rank)

    if world_size > 1:
        torch.cuda.set_device(local_rank)
        dist.init_process_group(backend="nccl", init_method="env://", rank=rank, world_size=world_size)

    if args.offload_model is None:
        args.offload_model = False if world_size > 1 else True
        if rank == 0: logging.info(f"Offload model automatically set to {args.offload_model}.")

    # 2. å‡†å¤‡è¾“å…¥æ•°æ® (åªåœ¨ä¸»è¿›ç¨‹ rank 0 ä¸­è¿›è¡Œ)
    image_paths = []
    prompts = []
    if rank == 0:
        logging.info(f"Loading inputs from project directory: {args.project_dir}")
        
        # æŸ¥æ‰¾æ‰€æœ‰åœºæ™¯å›¾ç‰‡
        images_dir = Path(args.project_dir) / "generated_images"
        image_paths = sorted(glob.glob(str(images_dir / "scene*.png")))
        if not image_paths:
            logging.error(f"âŒ No images found in {images_dir}")
            sys.exit(1)

        # è¯»å–æ‰€æœ‰æç¤ºè¯
        prompts_file = Path(args.project_dir) / "img2vid_prompts.txt"
        if not prompts_file.exists():
            logging.error(f"âŒ Prompts file not found at {prompts_file}")
            sys.exit(1)
        with open(prompts_file, 'r', encoding='utf-8') as f:
            prompts = [line.strip() for line in f if line.strip()]
        
        if len(image_paths) != len(prompts):
            logging.warning(f"âš ï¸ Mismatch: Found {len(image_paths)} images but {len(prompts)} prompts. Will process up to the smaller count.")
            
        num_scenes = min(len(image_paths), len(prompts))
        image_paths = image_paths[:num_scenes]
        prompts = prompts[:num_scenes]
        
        logging.info(f"âœ… Found {num_scenes} scenes to process.")
        
        # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨
        os.makedirs(args.output_dir, exist_ok=True)


    # 3. åŠ è½½æ¨¡å‹ (åªåŠ è½½ä¸€æ¬¡!)
    if rank == 0: logging.info("Loading Wan-AI I2V model... This may take a while.")
    
    # ä»»åŠ¡é…ç½®ç¡¬ç¼–ç ä¸º i2v-14B
    task = "i2v-14B" 
    cfg = WAN_CONFIGS[task]
    
    wan_i2v = wan.WanI2V(
        config=cfg,
        checkpoint_dir=args.ckpt_dir,
        device_id=device,
        rank=rank,
        t5_fsdp=args.t5_fsdp,
        dit_fsdp=args.dit_fsdp,
        use_usp=(args.ulysses_size > 1 or args.ring_size > 1),
        t5_cpu=args.t5_cpu,
    )
    if rank == 0: logging.info("âœ… Model loaded successfully.")

    # 4. å¾ªç¯ç”Ÿæˆæ‰€æœ‰è§†é¢‘ç‰‡æ®µ
    # ä¸»è¿›ç¨‹(rank 0)å°†ä»»åŠ¡åˆ†å‘ç»™æ‰€æœ‰è¿›ç¨‹
    for i in range(len(image_paths)):
        if rank == 0:
            scene_num = i + 1
            image_path = image_paths[i]
            prompt = prompts[i]
            
            logging.info("-" * 50)
            logging.info(f"ğŸ¬ Processing Scene {scene_num}/{len(image_paths)}")
            logging.info(f"  Image: {image_path}")
            logging.info(f"  Prompt: {prompt[:100]}...")

            img = Image.open(image_path).convert("RGB")
        
        # åœ¨æ‰€æœ‰è¿›ç¨‹ä¸­åŒæ­¥æ•°æ®ã€‚ä¸»è¿›ç¨‹å¹¿æ’­å›¾åƒå’Œæç¤ºã€‚
        # è¿™é‡Œæˆ‘ä»¬ç®€åŒ–å¤„ç†ï¼Œå‡è®¾å›¾åƒå’Œæç¤ºè¯ä¸éœ€è¦å¹¿æ’­ï¼Œå› ä¸ºæ¯ä¸ªrankéƒ½ä¼šè°ƒç”¨generate
        # ä½†åœ¨å®é™…åˆ†å¸ƒå¼ç¯å¢ƒä¸­ï¼Œæ›´å¥½çš„åšæ³•æ˜¯å¹¿æ’­æ•°æ®ã€‚ä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬æš‚æ—¶è·³è¿‡è¿™æ­¥ã€‚
        # æ³¨æ„ï¼šåœ¨å½“å‰è„šæœ¬ä¸­ï¼Œåªæœ‰ rank 0 æœ‰ image_paths å’Œ promptsï¼Œ
        # ä¸ºäº†è®©æ‰€æœ‰rankéƒ½èƒ½æ‰§è¡Œï¼Œæˆ‘ä»¬å°†åœ¨rank 0ä¸­æ‰§è¡Œç”Ÿæˆï¼Œç„¶åä¿å­˜ã€‚
        # è¿™æ˜¯ä¸€ä¸ªç®€åŒ–ï¼Œå¦‚æœéœ€è¦çœŸæ­£çš„åˆ†å¸ƒå¼æ¨ç†ï¼Œéœ€è¦å¹¿æ’­ image å’Œ promptã€‚

        if rank == 0:
            video_tensor = wan_i2v.generate(
                prompt,
                img,
                max_area=MAX_AREA_CONFIGS[args.size],
                frame_num=args.frame_num,
                shift=args.sample_shift,
                sample_solver='unipc',
                sampling_steps=args.sample_steps,
                guide_scale=args.sample_guide_scale,
                seed=args.base_seed + i,  # ä¸ºæ¯ä¸ªåœºæ™¯ä½¿ç”¨ä¸åŒçš„ç§å­ä»¥å¢åŠ å¤šæ ·æ€§
                offload_model=args.offload_model
            )

            # ä¿å­˜ç”Ÿæˆçš„è§†é¢‘ç‰‡æ®µ
            output_filename = f"scene_{i+1:02d}.mp4"
            save_path = os.path.join(args.output_dir, output_filename)
            logging.info(f"  Saving video to: {save_path}")
            
            cache_video(
                tensor=video_tensor[None],
                save_file=save_path,
                fps=cfg.sample_fps,
                normalize=True,
                value_range=(-1, 1)
            )
            logging.info(f"âœ… Scene {scene_num} completed.")

    if rank == 0:
        logging.info("-" * 50)
        logging.info("ğŸ‰ All video clips have been generated successfully!")


if __name__ == "__main__":
    main()